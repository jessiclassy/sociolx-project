{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96306cd-6681-4e27-91d3-227c01aeb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from geopy.geocoders import Nominatim\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d220e7-1a67-455e-a46a-7b3e99de0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL\n",
      "14 speakers\n",
      "12 speakers data used\n",
      "VLD\n",
      "14 speakers\n",
      "14 speakers data used\n",
      "DCB\n",
      "75 speakers\n",
      "71 speakers data used\n",
      "ROC\n",
      "20 speakers\n",
      "20 speakers data used\n",
      "PRV\n",
      "38 speakers\n",
      "38 speakers data used\n",
      "LES\n",
      "15 speakers\n",
      "15 speakers data used\n"
     ]
    }
   ],
   "source": [
    "# Check which speakers are represented in the data, then filter region_data by present speakers\n",
    "output = pd.read_csv(\"output/detected_quotatives_.csv\")\n",
    "region_data = dict()\n",
    "\n",
    "columns_to_use = set()\n",
    "for f in glob(\"data/*_metadata_*.txt\"):\n",
    "    region = os.path.basename(f)[0:3]\n",
    "    print(region)\n",
    "    \n",
    "    curr = pd.read_csv(f, sep=\"\\t\")\n",
    "    print(f\"{len(curr)} speakers\")\n",
    "    \n",
    "    curr = curr.loc[curr['CORAAL.Spkr'].isin(output.speaker_id.tolist())]\n",
    "    print(f\"{len(curr)} speakers data used\")\n",
    "    \n",
    "    if len(columns_to_use):\n",
    "        columns_to_use = columns_to_use & set(curr.columns.tolist())\n",
    "    else:\n",
    "        columns_to_use = set(curr.columns.tolist())\n",
    "        \n",
    "    region_data[region] = curr[list(columns_to_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2226a9b8-4909-4c5c-be0a-4ab386b98b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create cache\n",
    "cache_file = Path('geocode_cache.json')\n",
    "if cache_file.exists():\n",
    "    with open(cache_file) as f:\n",
    "        geocode_cache = json.load(f)\n",
    "else:\n",
    "    geocode_cache = {}\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"study_migration_viz\")\n",
    "\n",
    "def get_coords(location_str):\n",
    "    \"\"\"Get coordinates with caching\"\"\"\n",
    "    if location_str in geocode_cache:\n",
    "        return geocode_cache[location_str]\n",
    "    \n",
    "    try:\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "        location = geolocator.geocode(location_str)\n",
    "        if location:\n",
    "            coords = [location.latitude, location.longitude]\n",
    "        else:\n",
    "            coords = [None, None]\n",
    "    except:\n",
    "        coords = [None, None]\n",
    "    \n",
    "    geocode_cache[location_str] = coords\n",
    "    return coords\n",
    "\n",
    "# US state abbreviations and full names for detection\n",
    "US_STATES = {\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',\n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',\n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',\n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'DC'\n",
    "}\n",
    "\n",
    "US_STATE_NAMES = {\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado',\n",
    "    'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
    "    'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
    "    'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "    'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
    "    'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "    'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
    "    'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
    "    'West Virginia', 'Wisconsin', 'Wyoming', 'District of Columbia'\n",
    "}\n",
    "\n",
    "def split_multi_city(loc):\n",
    "    \"\"\"\n",
    "    Detect and split entries like \"Jackson, MO, Buffalo, NY\"\n",
    "    or \"Prince George's County, Maryland, Northern Virginia, Southern Maryland\"\n",
    "    Returns list of individual location entries\n",
    "    \"\"\"\n",
    "    parts = [p.strip() for p in loc.split(',')]\n",
    "    \n",
    "    # Single city-state pair - don't split\n",
    "    if len(parts) == 2:\n",
    "        return [loc]\n",
    "    \n",
    "    # Pattern 1: City, ST, City, ST (where ST is state abbrev)\n",
    "    if len(parts) >= 4 and len(parts) % 2 == 0:\n",
    "        potential_states = parts[1::2]\n",
    "        if all(p.upper() in US_STATES for p in potential_states):\n",
    "            # Split into city-state pairs\n",
    "            cities = []\n",
    "            for i in range(0, len(parts), 2):\n",
    "                cities.append(f\"{parts[i]}, {parts[i+1]}\")\n",
    "            return cities\n",
    "    \n",
    "    # Pattern 2: Location, State, Location, State (where State is full name)\n",
    "    if len(parts) >= 3:\n",
    "        potential_full_states = parts[1::2]\n",
    "        if all(p in US_STATE_NAMES for p in potential_full_states):\n",
    "            locations = []\n",
    "            for i in range(0, len(parts), 2):\n",
    "                if i+1 < len(parts):\n",
    "                    locations.append(f\"{parts[i]}, {parts[i+1]}\")\n",
    "                else:\n",
    "                    locations.append(parts[i])\n",
    "            return locations\n",
    "    \n",
    "    # Pattern 3: Multiple locations with regional prefixes\n",
    "    regional_prefixes = ['Northern', 'Southern', 'Eastern', 'Western', 'Central',\n",
    "                        'North', 'South', 'East', 'West', 'Upper', 'Lower']\n",
    "    \n",
    "    locations = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        part = parts[i].strip()\n",
    "        \n",
    "        if i + 1 < len(parts):\n",
    "            next_part = parts[i + 1].strip()\n",
    "            combined = f\"{part} {next_part}\"\n",
    "            \n",
    "            if any(part.startswith(prefix) for prefix in regional_prefixes):\n",
    "                if next_part in US_STATE_NAMES or next_part.upper() in US_STATES:\n",
    "                    locations.append(combined)\n",
    "                    i += 2\n",
    "                    continue\n",
    "        \n",
    "        locations.append(part)\n",
    "        i += 1\n",
    "    \n",
    "    if len(locations) > 1:\n",
    "        return locations\n",
    "    \n",
    "    return [loc]\n",
    "\n",
    "loc_map = {\n",
    "    \"LES\": 'Lower East Side, NYC, NY',\n",
    "    \"PG County\": \"Prince George's County, MD\",\n",
    "    'Harrisburg (3 years)': \"Harrisburg, PA\",\n",
    "    \"Harlem\": 'Harlem, NYC, NY',\n",
    "}\n",
    "\n",
    "def clean_location(loc):\n",
    "    \"\"\"Clean location string and validate\"\"\"\n",
    "    if not loc or pd.isna(loc):\n",
    "        return []\n",
    "    \n",
    "    loc = str(loc).strip()\n",
    "    \n",
    "    # Remove parenthetical info like \"(7 years)\" or \"(military)\"\n",
    "    loc = re.sub(r'\\s*\\([^)]*\\)', '', loc)\n",
    "    loc = loc.strip()\n",
    "    \n",
    "    # Skip if empty after cleaning\n",
    "    if not loc:\n",
    "        return []\n",
    "    \n",
    "    # Apply known corrections for ambiguous strings\n",
    "    if loc in loc_map:\n",
    "        loc = loc_map[loc]\n",
    "    \n",
    "    # Try to split multi-city entries\n",
    "    locations = split_multi_city(loc)\n",
    "\n",
    "    valid_locations = []\n",
    "    for single_loc in locations:\n",
    "        single_loc = single_loc.strip()\n",
    "        \n",
    "        # Skip vague county abbreviations like \"PG County\" without full name\n",
    "        if re.search(r'County$', single_loc, re.IGNORECASE):\n",
    "            if ',' not in single_loc and len(single_loc.split()) <= 2:\n",
    "                continue\n",
    "        \n",
    "        # Skip just state abbreviation like \"FL\"\n",
    "        if re.match(r'^[A-Z]{2}$', single_loc):\n",
    "            continue\n",
    "        \n",
    "        # Skip non-US locations (optional)\n",
    "        non_us = ['Germany', 'Japan', 'Korea', 'England', 'France', 'Italy', 'Spain']\n",
    "        if any(country in single_loc for country in non_us):\n",
    "            continue\n",
    "        \n",
    "        # Handle \"Northern Virginia\" type entries - treat as just the state\n",
    "        regional_prefixes = ['Northern', 'Southern', 'Eastern', 'Western', 'Central',\n",
    "                            'North', 'South', 'East', 'West', 'Upper', 'Lower']\n",
    "        \n",
    "        for prefix in regional_prefixes:\n",
    "            if single_loc.startswith(prefix + ' '):\n",
    "                # Extract the state name\n",
    "                state_part = single_loc[len(prefix)+1:].strip()\n",
    "                if state_part in US_STATE_NAMES:\n",
    "                    single_loc = state_part + ', USA'\n",
    "                    break\n",
    "        \n",
    "        # Handle entries without commas\n",
    "        if ',' not in single_loc:\n",
    "            # Check if it's a US state name (accept it)\n",
    "            if single_loc in US_STATE_NAMES:\n",
    "                single_loc = single_loc + ', USA'\n",
    "            else:\n",
    "                # Skip single-word vague entries\n",
    "                if len(single_loc.split()) == 1:\n",
    "                    continue\n",
    "                # Multi-word entries without commas - add USA\n",
    "                single_loc = single_loc + ', USA'\n",
    "        \n",
    "        valid_locations.append(single_loc)\n",
    "    \n",
    "    return valid_locations\n",
    "\n",
    "# City abbreviation mapping\n",
    "city_map = {\n",
    "    'ATL': 'Atlanta, GA',\n",
    "    'DCB': 'Washington, D.C., USA',\n",
    "    'LES': 'Lower East Side, NYC, NY',\n",
    "    'ROC': 'Rochester, NY',\n",
    "    'PRV': 'Princeville, NC',\n",
    "    'VLD': 'Valdosta, GA'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcff8319-93b0-40d5-bd2b-9591063a86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 133 valid locations\n",
      "Skipped 43 invalid/vague locations:\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'Germany' (from: Germany)\n",
      "  - 'Germany' (from: Germany)\n",
      "  - 'Harrisburg (3 years)' (from: Harrisburg (3 years))\n",
      "  - 'Germany (military)' (from: Germany (military))\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'unknown' (from: unknown)\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "  - 'Harlem (36 years)' (from: Harlem (36 years))\n",
      "  - 'Harlem (36 years)' (from: Harlem (36 years))\n",
      "  - 'none' (from: none)\n",
      "  - 'none' (from: none)\n",
      "\n",
      "Example valid locations:\n",
      "  - New York, NY → Atlanta, GA\n",
      "  - Bridgeport, CT → Atlanta, GA\n",
      "  - Stone Mountain, GA → Atlanta, GA\n",
      "  - Newton, GA → Atlanta, GA\n",
      "  - Atlanta, GA → Atlanta, GA\n",
      "  - Milwaukee, WI → Atlanta, GA\n",
      "  - Inglewood, CA → Atlanta, GA\n",
      "  - Maryland, USA → Atlanta, GA\n",
      "  - Freehold, NJ → Atlanta, GA\n",
      "  - Lakewood, NJ → Atlanta, GA\n",
      "\n",
      "Geocoding 75 unique locations...\n",
      "\n",
      "Successfully mapped 118 migration paths\n"
     ]
    }
   ],
   "source": [
    "# Transform: expand all migrations\n",
    "rows = []\n",
    "skipped = []\n",
    "\n",
    "for residence_abbrev, df in region_data.items():\n",
    "    current = city_map.get(residence_abbrev, residence_abbrev)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Skip empty values\n",
    "        if pd.isna(row['Other.Places.Lived']) or row['Other.Places.Lived'] == '':\n",
    "            continue\n",
    "\n",
    "        speaker_id = row['CORAAL.Spkr']\n",
    "        \n",
    "        original = row['Other.Places.Lived']\n",
    "        \n",
    "        # First split on semicolons (primary delimiter)\n",
    "        semicolon_parts = [p.strip() for p in str(original).split(';')]\n",
    "        \n",
    "        for part in semicolon_parts:\n",
    "            # Then try to clean and split each part\n",
    "            cleaned_locations = clean_location(part)\n",
    "            \n",
    "            if cleaned_locations:\n",
    "                for cleaned in cleaned_locations:\n",
    "                    rows.append({\n",
    "                        'speaker_id': speaker_id,\n",
    "                        'previous': cleaned, \n",
    "                        'current': current, \n",
    "                        'original': original\n",
    "                    })\n",
    "            else:\n",
    "                skipped.append({'location': part, 'original': original, 'speaker_id': speaker_id})\n",
    "\n",
    "df_expanded = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"Processed {len(rows)} valid locations\")\n",
    "print(f\"Skipped {len(skipped)} invalid/vague locations:\")\n",
    "for skip in skipped: \n",
    "    print(f\"  - '{skip['location']}' (from: {skip['original']})\")\n",
    "\n",
    "# Show some examples of what was kept\n",
    "if len(df_expanded) > 0:\n",
    "    print(f\"\\nExample valid locations:\")\n",
    "    for _, row in df_expanded.head(10).iterrows():\n",
    "        print(f\"  - {row['previous']} → {row['current']}\")\n",
    "\n",
    "# Geocode all unique locations\n",
    "all_locations = set(df_expanded['previous']) | set(df_expanded['current'])\n",
    "print(f\"\\nGeocoding {len(all_locations)} unique locations...\")\n",
    "\n",
    "failed_geocodes = []\n",
    "for loc in all_locations:\n",
    "    if loc not in geocode_cache:\n",
    "        print(f\"Geocoding: {loc}\")\n",
    "        coords = get_coords(loc)\n",
    "        if coords == [None, None]:\n",
    "            failed_geocodes.append(loc)\n",
    "\n",
    "# Save cache\n",
    "with open(cache_file, 'w') as f:\n",
    "    json.dump(geocode_cache, f, indent=2)\n",
    "\n",
    "if failed_geocodes:\n",
    "    print(f\"\\nFailed to geocode {len(failed_geocodes)} locations:\")\n",
    "    for loc in failed_geocodes:\n",
    "        print(f\"  - {loc}\")\n",
    "\n",
    "# Add coordinates to dataframe\n",
    "df_expanded['prev_lat'] = df_expanded['previous'].apply(lambda x: get_coords(x)[0])\n",
    "df_expanded['prev_lon'] = df_expanded['previous'].apply(lambda x: get_coords(x)[1])\n",
    "df_expanded['curr_lat'] = df_expanded['current'].apply(lambda x: get_coords(x)[0])\n",
    "df_expanded['curr_lon'] = df_expanded['current'].apply(lambda x: get_coords(x)[1])\n",
    "\n",
    "# Remove rows with missing coordinates\n",
    "df_expanded = df_expanded.dropna()\n",
    "\n",
    "print(f\"\\nSuccessfully mapped {len(df_expanded)} migration paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd019da9-737f-4c78-bbac-17a128ffa0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    70.000000\n",
       "mean      1.685714\n",
       "std       1.173905\n",
       "min       1.000000\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       2.000000\n",
       "max       7.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expanded.speaker_id.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70351e3-17f1-4dab-a3ce-e9346b3f869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating visualization with 118 paths...\n",
      "Checking for NaN values:\n",
      "  prev_lat: 0\n",
      "  prev_lon: 0\n",
      "  curr_lat: 0\n",
      "  curr_lon: 0\n"
     ]
    }
   ],
   "source": [
    "# Create map with color coding by current residence\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create color mapping\n",
    "unique_residences = sorted(df_expanded['current'].unique())\n",
    "n_residences = len(unique_residences)\n",
    "colors = px.colors.sample_colorscale(\n",
    "    \"plasma\", \n",
    "    [n/(n_residences -1) for n in range((n_residences))]\n",
    ")\n",
    "\n",
    "color_map = {}\n",
    "\n",
    "for i, res in enumerate(unique_residences):\n",
    "    color_map[res] = colors[i]\n",
    "\n",
    "print(f\"\\nCreating visualization with {len(df_expanded)} paths...\")\n",
    "\n",
    "# Verify no NaN values slipped through\n",
    "print(f\"Checking for NaN values:\")\n",
    "print(f\"  prev_lat: {df_expanded['prev_lat'].isna().sum()}\")\n",
    "print(f\"  prev_lon: {df_expanded['prev_lon'].isna().sum()}\")\n",
    "print(f\"  curr_lat: {df_expanded['curr_lat'].isna().sum()}\")\n",
    "print(f\"  curr_lon: {df_expanded['curr_lon'].isna().sum()}\")\n",
    "\n",
    "# Add lines grouped by current residence\n",
    "for current_res in unique_residences:\n",
    "    df_subset = df_expanded[df_expanded['current'] == current_res]\n",
    "    \n",
    "    # Collect all line segments for this residence\n",
    "    line_lons = []\n",
    "    line_lats = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        line_lons.extend([row['prev_lon'], row['curr_lon'], None])\n",
    "        line_lats.extend([row['prev_lat'], row['curr_lat'], None])\n",
    "    \n",
    "    # Add as single trace per residence\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=line_lons,\n",
    "        lat=line_lats,\n",
    "        mode='lines',\n",
    "        line=dict(width=1.5, color=color_map[current_res]),\n",
    "        opacity=0.6,\n",
    "        showlegend=False,\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "# Add origin points (gray)\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    lon=df_expanded['prev_lon'].tolist(),\n",
    "    lat=df_expanded['prev_lat'].tolist(),\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, color='lightgray', line=dict(width=0.5, color='white')),\n",
    "    text=df_expanded['previous'].tolist(),\n",
    "    name='Other places lived',\n",
    "    hovertemplate='<b>%{text}</b><extra></extra>'\n",
    "))\n",
    "\n",
    "# Add current location points with halos for high traffic\n",
    "current_counts = df_expanded.groupby(['current', 'curr_lat', 'curr_lon']).size().reset_index(name='count')\n",
    "max_count = current_counts['count'].max()\n",
    "\n",
    "for _, row in current_counts.iterrows():\n",
    "    # Add outer halo for destinations with multiple paths\n",
    "    if row['count'] > 1:\n",
    "        fig.add_trace(go.Scattergeo(\n",
    "            lon=[row['curr_lon']],\n",
    "            lat=[row['curr_lat']],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=14 + row['count'] * 2,\n",
    "                color=color_map[row['current']],\n",
    "                opacity=0.3,\n",
    "                line=dict(width=0)\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "    \n",
    "    # Add main marker\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[row['curr_lon']],\n",
    "        lat=[row['curr_lat']],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=14,\n",
    "            color=color_map[row['current']],\n",
    "            line=dict(width=2, color='white')\n",
    "        ),\n",
    "        text=row['current'],\n",
    "        name=row['current'],\n",
    "        hovertemplate=f\"<b>{row['current']}</b><br>Participants: {row['count']}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=None,\n",
    "    geo=dict(\n",
    "        scope='usa',\n",
    "        projection_type='albers usa',\n",
    "        showland=True,\n",
    "        landcolor='rgb(250, 250, 250)',\n",
    "        showlakes=True,\n",
    "        lakecolor='rgb(255, 255, 255)',\n",
    "        coastlinecolor='rgb(200, 200, 200)',\n",
    "        # Add these for state borders:\n",
    "        showsubunits=True,  # Show state boundaries\n",
    "        subunitcolor='rgb(150, 150, 150)',  # State border color\n",
    "        subunitwidth=0.5,  # State border thickness\n",
    "        showcountries=False,\n",
    "        showframe=False\n",
    "    ),\n",
    "    width=800,   # Standard page width\n",
    "    height=550,  # Good aspect ratio for letter/A4\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        title=None,\n",
    "        yanchor=\"top\",\n",
    "        y=0.98,\n",
    "        xanchor=\"left\",\n",
    "        x=0.02,\n",
    "        font=dict(size=20),\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "        bordercolor='rgb(250, 250, 250)',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    font=dict(size=10, color='black', family='Arial'),\n",
    "    paper_bgcolor='white',\n",
    "    margin=dict(l=10, r=10, t=50, b=10)\n",
    ")\n",
    "\n",
    "# Also adjust marker and line sizes for print clarity\n",
    "# In the lines section:\n",
    "line=dict(width=1, color=color_map[current_res]),  # Thinner lines\n",
    "\n",
    "# In the origin points:\n",
    "marker=dict(size=4, color='lightgray', line=dict(width=0.3, color='white')),  # Smaller\n",
    "\n",
    "# In the current location points:\n",
    "marker=dict(\n",
    "    size=10,  # Slightly smaller\n",
    "    color=color_map[row['current']],\n",
    "    line=dict(width=1.5, color='white')\n",
    ")\n",
    "# fig.show()\n",
    "fig.write_image(\"viz/migration_map.png\", scale=2, width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0331e98f-14c9-4e52-85c2-afb6be3b9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Aggregate paths by (previous, current) to get counts\n",
    "path_counts = df_expanded.groupby(['previous', 'current', 'prev_lat', 'prev_lon', 'curr_lat', 'curr_lon']).size().reset_index(name='path_count')\n",
    "\n",
    "# Create map\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create color mapping for destinations\n",
    "unique_residences = df_expanded['current'].unique()\n",
    "n_residences = len(unique_residences)\n",
    "\n",
    "colors = px.colors.sample_colorscale(\n",
    "    \"portland\", \n",
    "    [n/(n_residences -1) for n in range(n_residences)]\n",
    ")\n",
    "\n",
    "color_map = {res: colors[i] for i, res in enumerate(unique_residences)}\n",
    "\n",
    "# Define scaling functions\n",
    "def get_line_width(count, min_count=1, max_count=7):\n",
    "    \"\"\"Scale line width from 0.5 to 6 based on path count\"\"\"\n",
    "    if max_count == min_count:\n",
    "        return 3\n",
    "    normalized = (count - min_count) / (max_count - min_count)\n",
    "    return 0.5 + normalized * 5.5\n",
    "\n",
    "def get_opacity(count, min_count=1, max_count=7):\n",
    "    \"\"\"Scale opacity from 0.25 to 0.9 based on path count\"\"\"\n",
    "    if max_count == min_count:\n",
    "        return 0.6\n",
    "    normalized = (count - min_count) / (max_count - min_count)\n",
    "    return 0.25 + normalized * 0.65\n",
    "\n",
    "min_path_count = path_counts['path_count'].min()\n",
    "max_path_count = path_counts['path_count'].max()\n",
    "\n",
    "# Add lines grouped by current residence (for consistent coloring)\n",
    "for current_res in unique_residences:\n",
    "    df_subset = path_counts[path_counts['current'] == current_res]\n",
    "    \n",
    "    # Add each path as a separate trace to control width/opacity individually\n",
    "    for _, row in df_subset.iterrows():\n",
    "        line_width = get_line_width(row['path_count'], min_path_count, max_path_count)\n",
    "        line_opacity = get_opacity(row['path_count'], min_path_count, max_path_count)\n",
    "        \n",
    "        fig.add_trace(go.Scattergeo(\n",
    "            lon=[row['prev_lon'], row['curr_lon']],\n",
    "            lat=[row['prev_lat'], row['curr_lat']],\n",
    "            mode='lines',\n",
    "            line=dict(width=line_width, color=color_map[current_res]),\n",
    "            opacity=line_opacity,\n",
    "            showlegend=False,\n",
    "            hovertemplate=f\"<b>{row['previous']} → {row['current']}</b><br>Participants: {row['path_count']}<extra></extra>\"\n",
    "        ))\n",
    "\n",
    "# Add origin points (gray)\n",
    "origin_points = path_counts.groupby(['previous', 'prev_lat', 'prev_lon']).size().reset_index(name='count')\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    lon=origin_points['prev_lon'].tolist(),\n",
    "    lat=origin_points['prev_lat'].tolist(),\n",
    "    mode='markers',\n",
    "    marker=dict(size=4, color='lightgray', line=dict(width=0.3, color='white')),\n",
    "    text=origin_points['previous'].tolist(),\n",
    "    name='Other places lived',\n",
    "    hovertemplate='<b>%{text}</b><extra></extra>'\n",
    "))\n",
    "\n",
    "# Add current location points with halos for high traffic\n",
    "current_counts = df_expanded.groupby(['current', 'curr_lat', 'curr_lon']).size().reset_index(name='count')\n",
    "\n",
    "for _, row in current_counts.iterrows():\n",
    "    # Add outer halo for destinations with multiple paths\n",
    "    if row['count'] > 1:\n",
    "        fig.add_trace(go.Scattergeo(\n",
    "            lon=[row['curr_lon']],\n",
    "            lat=[row['curr_lat']],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=12 + row['count'] * 0.8,\n",
    "                color=color_map[row['current']],\n",
    "                opacity=0.2,\n",
    "                line=dict(width=0)\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "    \n",
    "    # Add main marker\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[row['curr_lon']],\n",
    "        lat=[row['curr_lat']],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=color_map[row['current']],\n",
    "            line=dict(width=1.5, color='white')\n",
    "        ),\n",
    "        text=row['current'],\n",
    "        name=row['current'],\n",
    "        hovertemplate=f\"<b>{row['current']}</b><br>Participants: {row['count']}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=None,\n",
    "    geo=dict(\n",
    "        scope='usa',\n",
    "        projection_type='albers usa',\n",
    "        showland=True,\n",
    "        landcolor='rgb(250, 250, 250)',\n",
    "        showlakes=True,\n",
    "        lakecolor='rgb(255, 255, 255)',\n",
    "        coastlinecolor='rgb(200, 200, 200)',\n",
    "        showsubunits=True,\n",
    "        subunitcolor='rgb(150, 150, 150)',\n",
    "        subunitwidth=0.5,\n",
    "        showcountries=False,\n",
    "        showframe=False\n",
    "    ),\n",
    "    width=800,\n",
    "    height=550,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        title=None,\n",
    "        yanchor=\"top\",\n",
    "        y=0.98,\n",
    "        xanchor=\"left\",\n",
    "        x=0.02,\n",
    "        font=dict(size=20),\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "        bordercolor='rgb(250, 250, 250)',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    font=dict(size=10, color='black', family='Arial'),\n",
    "    paper_bgcolor='white',\n",
    "    margin=dict(l=10, r=10, t=50, b=10)\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "fig.write_image(\"viz/migration_map.png\", scale=3, width=800, height=600)\n",
    "# print(\"\\nMap saved to viz/migration_map.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
