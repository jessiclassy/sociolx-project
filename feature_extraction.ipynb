{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0449ac-5942-4cf4-aeac-4aacb99a7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eab92d-e680-4824-b5b4-b4727e93e25d",
   "metadata": {},
   "source": [
    "# Globals\n",
    "\n",
    "Load data, define regexes and repeated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcd171d-8444-4b00-9ff1-6e1324bcc86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load quotative constructions\n",
    "data = pd.read_csv(\"output/detected_quotatives_.csv\")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328cb308-f11d-439f-ba38-c79a067cf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_RE = re.compile(r\"(so|but|now|and)\", flags=re.IGNORECASE)\n",
    "\n",
    "PER_RE = {\n",
    "    \"1st\": r\"(\\bI|we\\b)\",\n",
    "    \"3rd\": r\"(\\bs?he|they\\b)\",\n",
    "}\n",
    "\n",
    "NUM_RE = {\n",
    "    \"SG\": r\"(\\bI|s?he\\b)\",\n",
    "    \"PL\": r\"(\\bwe|they\\b)\"\n",
    "}\n",
    "\n",
    "q_copula_forms = [\n",
    "    \"q_like_copula\",\n",
    "    \"q_like_contracted\",\n",
    "    \"q_like_zero\",\n",
    "    \"q_all_copula\",\n",
    "    \"q_all_contracted\",\n",
    "    \"q_all_zero\"\n",
    "]\n",
    "\n",
    "gen_suffix = r\"((,|-)|[<\\(\\/\\[]{1})\"\n",
    "opt_like_suffix = r\"(?:like)\"\n",
    "opt_intj_suffix = r\"\\b(ah|ay|eh|ha|hey|hm|huh|man|mm|oh|uh|um|yo|wow)\"\n",
    "\n",
    "suffix_patterns = {\n",
    "    \"like_all\": [gen_suffix, opt_intj_suffix],\n",
    "    \"other\": [gen_suffix, opt_like_suffix, opt_intj_suffix]\n",
    "}\n",
    "\n",
    "optional_suffixes = [opt_like_suffix, opt_intj_suffix]\n",
    "all_suffixes = [opt_like_suffix, opt_intj_suffix, gen_suffix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f3bdf0-8083-4662-9662-5e07dff2f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step - add spaces before punctuation\n",
    "def tokenize_with_punctuation(text):\n",
    "    # Add space before trailing punctuation\n",
    "    text = re.sub(gen_suffix, r\" \\1\", text)\n",
    "    # Add space before punctuation attached to words\n",
    "    text = re.sub(fr\"(\\w+){gen_suffix}\", r'\\1 \\2', text)\n",
    "    return [s for s in text.split() if len(s)]\n",
    "    \n",
    "def find_regex(pat, s):\n",
    "    search_result = re.search(pat, s)\n",
    "    if search_result:\n",
    "        return search_result.group(0)\n",
    "    else:\n",
    "        return \"###\"\n",
    "\n",
    "def update_target(l):\n",
    "    if len(l):\n",
    "        return l[1:]\n",
    "\n",
    "def extract_verb_construction(row):\n",
    "    \"\"\"\n",
    "    Remove suffix tokens from temp based on q_type and suffix content.\n",
    "    \n",
    "    Returns the verb construction without suffix elements.\n",
    "    \"\"\"\n",
    "    temp_list = row['temp'].copy()\n",
    "    suffix = row['q_suffix']\n",
    "    q_type = row['q_type']  \n",
    "\n",
    "    # Remove suffixes if they have been detected\n",
    "    for s_type in all_suffixes:\n",
    "        if re.match(s_type, suffix):\n",
    "            temp_list = [item for item in temp_list if not re.search(s_type, item)]\n",
    "\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10045bd-2317-4a61-ba43-204d50ce1065",
   "metadata": {},
   "source": [
    "# Main data processing\n",
    "Extract components with quotative constructions, assign to display names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835d6e6f-1f9d-4ded-96f2-7103cea796d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2789\n",
      "2789\n"
     ]
    }
   ],
   "source": [
    "# create copy of data\n",
    "output = data.copy()\n",
    "print(len(output))\n",
    "# create temp column with list of non-empty tokens to parse\n",
    "output[\"temp\"] = output.target.apply(tokenize_with_punctuation)\n",
    "\n",
    "# extract if the first element matches the known set of discourse prefixes (T/F)\n",
    "output[\"has_prefix\"] = output.temp.apply(\n",
    "    lambda t: bool(PREFIX_RE.search(t[0]))\n",
    ")\n",
    "\n",
    "# drop this element from the temp list column\n",
    "# temp should now only have subject + q_form + suffix\n",
    "output.loc[output[\"has_prefix\"], \"temp\"] = output.loc[output[\"has_prefix\"], \"temp\"].copy().apply(update_target)\n",
    "\n",
    "# extract subject value from temp\n",
    "output[\"subj_val\"] = output.temp.apply(\n",
    "    lambda t: t[0]\n",
    ")\n",
    "\n",
    "# extract subject person from value\n",
    "for subj_per in PER_RE.keys():\n",
    "    output[f\"subj_{subj_per}\"] = output.subj_val.apply(\n",
    "        lambda sv: bool(re.search(PER_RE[subj_per], sv, flags=re.IGNORECASE))\n",
    "    )\n",
    "\n",
    "# extract subject number from value\n",
    "for subj_num in NUM_RE.keys():\n",
    "    output[f\"subj_{subj_num}\"] = output.subj_val.apply(\n",
    "        lambda sv: bool(re.search(NUM_RE[subj_num], sv, flags=re.IGNORECASE))\n",
    "    )\n",
    "\n",
    "# temp should now only be q_form + suffix; remove the 1st element\n",
    "output.temp = output.temp.copy().apply(update_target)\n",
    "\n",
    "# split data into q_like_*, q_all_* vs. all others since they allow different suffixes\n",
    "output_split = {\n",
    "    \"like_all\": output.loc[output.q_type.isin(q_copula_forms)],\n",
    "    \"other\": output.loc[~output.q_type.isin(q_copula_forms)]\n",
    "}\n",
    "\n",
    "# Separate suffix from main verb construction\n",
    "for q_type in output_split:\n",
    "    curr = output_split[q_type].copy()\n",
    "    \n",
    "    # Get the appropriate pattern\n",
    "    pattern = suffix_patterns.get(q_type, suffix_patterns[\"other\"])\n",
    "    \n",
    "    # Extract suffix and update temp\n",
    "    curr[\"q_suffix\"] = curr.temp.apply(lambda t: find_regex(r\"|\".join(pattern), t[-1]))\n",
    "    # curr[\"temp\"] = curr[\"temp\"].\n",
    "    # Remove suffix sequence from remaining target based on q_type and detected interjections, etc.\n",
    "    curr[\"temp\"] = curr.apply(extract_verb_construction, axis=1)\n",
    "    # Reassign the DF\n",
    "    output_split[q_type] = curr\n",
    "\n",
    "# Reassign total output to the split data as well\n",
    "output = pd.concat(output_split.values())\n",
    "\n",
    "# Map q_type to display names\n",
    "verb_mapping = {\n",
    "    'q_say': 'say',\n",
    "    'q_like_copula': 'be+like',\n",
    "    'q_like_contracted': 'be+like',\n",
    "    'q_like_zero': 'be+like',\n",
    "    'q_go': 'go',\n",
    "    'q_ask': 'other',\n",
    "    'q_repeat': 'other',\n",
    "    'q_think': 'other',\n",
    "    'q_tell': 'other',\n",
    "    'q_all_zero': 'other',\n",
    "    'q_all_copula': 'other',\n",
    "    'q_all_contracted': 'other'\n",
    "}\n",
    "output['verb_display'] = output['q_type'].map(verb_mapping)\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6dc9db-17b6-4ae6-aecf-ad33d9a5601b",
   "metadata": {},
   "source": [
    "# Incorporating speaker variables\n",
    "\n",
    "Read speaker metadata, rename columns for easy merging, reproducing figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c1cfcd-17b8-4304-8a77-49f70e878249",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'Age', # used to create unified age cohorts\n",
    "    'Age.Group', # original age cohorts, depends on corpus\n",
    "    'Gender',\n",
    "    'Other.Places.Lived',\n",
    "    'Year.of.Birth',\n",
    "    'Year.of.Interview',\n",
    "    'region_id',\n",
    "    'speaker_id',\n",
    "    'source_file'\n",
    "]\n",
    "\n",
    "# Procure metadata\n",
    "metadata = dict()\n",
    "for f in glob(\"data/*_metadata_*.txt\"):\n",
    "    curr = pd.read_csv(f, sep=\"\\t\", dtype ={'Age.Group': str})\n",
    "    # Assign region ID\n",
    "    curr[\"region_id\"] = os.path.basename(f)[0:3]\n",
    "    \n",
    "    # Filter by relevant speakers\n",
    "    curr = curr.loc[curr['CORAAL.Spkr'].isin(output.speaker_id.tolist())]\n",
    "\n",
    "    curr = curr.rename(\n",
    "        columns={\n",
    "            'CORAAL.Spkr': 'speaker_id',\n",
    "            'CORAAL.File': 'source_file'\n",
    "        }\n",
    "    )\n",
    "    # Filter by relevant columns\n",
    "    metadata[f] = curr[columns_to_use]\n",
    "\n",
    "# Flatten metadata and generate new age cohorts\n",
    "spkr_df = pd.concat(metadata.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d20ce9-470a-4d6d-a47d-5fcfe98377f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Failed to match: 3 rows\n",
      "\n",
      "  - 1 unique speaker_ids need fallback\n",
      "\n",
      "2789\n"
     ]
    }
   ],
   "source": [
    "# Merge metadata content onto quotative productions\n",
    "df = output.merge(\n",
    "    spkr_df,\n",
    "    on=['source_file', 'speaker_id', 'region_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check if any rows failed to merge\n",
    "initial_matches = df[columns_to_use[0]].notna().sum()\n",
    "print(f\"  - Failed to match: {len(df) - initial_matches} rows\\n\")\n",
    "\n",
    "# Identify speaker_ids that need fallback\n",
    "needs_fallback = df[df[columns_to_use[0]].isna()]['speaker_id'].unique()\n",
    "print(f\"  - {len(needs_fallback)} unique speaker_ids need fallback\\n\")\n",
    "\n",
    "# For rows still missing demographics, do a fallback merge on speaker_id only\n",
    "missing_mask = df[columns_to_use[0]].isna()\n",
    "\n",
    "if missing_mask.any():\n",
    "    # Get unique metadata for each speaker_id (keep first occurrence)\n",
    "    metadata_lookup = spkr_df.drop_duplicates(\n",
    "        subset='speaker_id', \n",
    "        keep='first'\n",
    "    )\n",
    "# For rows with missing data, look up by speaker_id\n",
    "    missing_speakers = df.loc[missing_mask, 'speaker_id']\n",
    "    fallback_data = missing_speakers.to_frame().merge(\n",
    "        metadata_lookup,\n",
    "        on='speaker_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill in the missing demographic columns\n",
    "    for col in columns_to_use:\n",
    "        df.loc[missing_mask, col] = fallback_data[col].values\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c14b72-23a0-4574-a620-87c7c8661a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_cohort(age):\n",
    "    if age < 29:\n",
    "        return '-29'\n",
    "    elif age > 50:\n",
    "        return '51+'\n",
    "    else:\n",
    "        return '30-50'   \n",
    "\n",
    "# Create age cohorts\n",
    "def assign_age_group(age):\n",
    "    if age <= 12:\n",
    "        return \"9-12\"\n",
    "    elif age >=13 and age <=16:\n",
    "        return \"13-16\"\n",
    "    elif age >=17 and age <=29:\n",
    "        return \"17-29\"\n",
    "    elif age >=30 and age <=39:\n",
    "        return \"30-39\"\n",
    "    elif age >=40 and age <=49:\n",
    "        return \"40-49\"\n",
    "    elif age >=50 and age <=59:\n",
    "        return \"50-59\"\n",
    "    elif age >=60:\n",
    "        return \"60+\"\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "# Create age cohorts\n",
    "def assign_cohort(year_of_birth):\n",
    "    if year_of_birth < 1941:\n",
    "        return \"Before 1941\"\n",
    "    if 1941 <= year_of_birth <= 1960:\n",
    "        return \"Cohort 1\\n1941-60\"\n",
    "    elif 1961 <= year_of_birth <= 1977:\n",
    "        return \"Cohort 2\\n1961-77\"\n",
    "    elif 1978 <= year_of_birth <= 1990:\n",
    "        return \"Cohort 3\\n1978-90\"\n",
    "    elif 1991 <= year_of_birth <= 2002:\n",
    "        return \"Cohort 4\\n1991-2002\"\n",
    "    elif year_of_birth > 2002:\n",
    "        return \"After 2002\"\n",
    "\n",
    "# Assign age cohorts in 3 ways\n",
    "# unified given CORAAL conventions\n",
    "df['Age.Cohort'] = df['Age'].apply(get_age_cohort)\n",
    "# based on T&D (2009)\n",
    "df['age_group'] = df['Age'].apply(assign_age_group)\n",
    "# based on Cukor-Avila (2012)\n",
    "df['cohort'] = df['Year.of.Birth'].apply(assign_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2e06eb-c4e0-42c0-b82b-2ed00d0864fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spkrs = df.drop_duplicates(subset='speaker_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7050e5b7-b698-445a-8efc-c34c85f29804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_type\n",
       "q_say                970\n",
       "q_like_copula        885\n",
       "q_like_contracted    528\n",
       "q_think              188\n",
       "q_tell                97\n",
       "q_go                  46\n",
       "q_like_zero           45\n",
       "q_all_zero            16\n",
       "q_all_copula           7\n",
       "q_ask                  5\n",
       "q_all_contracted       1\n",
       "q_repeat               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.q_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d2316f4-a414-44b0-a976-b0e49b401681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/coded.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
